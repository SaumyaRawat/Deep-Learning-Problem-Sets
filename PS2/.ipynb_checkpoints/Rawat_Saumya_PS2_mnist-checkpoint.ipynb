{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(\"MNIST/trainImages.npy\")\n",
    "test_data = np.load(\"MNIST/testImages.npy\")\n",
    "\n",
    "y_train = np.load(\"MNIST/trainLabels.npy\")\n",
    "y_test = np.load(\"MNIST/testLabels.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data\n",
    "1. Standardize dataset\n",
    "2. Ensure shape is compatible with required format for tensorflow:  (batch, height, width, channels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set:  (60000, 28, 28, 1)\n",
      "Some examples of the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Normalize\n",
    "x_train = train_data[:,0:]/255\n",
    "x_test= test_data[:,0:]/255\n",
    "\n",
    "# Ensure Correct shape\n",
    "image_rows = 28\n",
    "image_cols = 28\n",
    "image_shape = (image_rows,image_cols,1)\n",
    "x_train = x_train.reshape(train_data.shape[0],*image_shape)\n",
    "x_test = x_test.reshape(test_data.shape[0],*image_shape)\n",
    "\n",
    "\n",
    "print(\"Size of training set: \",x_train.shape)\n",
    "\n",
    "\n",
    "print(\"Some examples of the training set:\")\n",
    "\n",
    "for i in range(0, 3):\n",
    "    plt.subplot(330 + (i+1))\n",
    "    plt.imshow(x_train[i].reshape(28,28), cmap=plt.get_cmap('gray'))\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define fully connected CNN\n",
    "\n",
    "### The three things I need to define in the CNN are:\n",
    "\n",
    "1. Convolution\n",
    "2. Activation\n",
    "3. Pooling\n",
    "\n",
    "In the first layer, 32 filters of (5, 5) size  with ReLU activation function is chosen to reduce training time and avoid  vanishing gradients. To reduce overfitting and training time, a MaxPooling layer is used to down-sample the input\n",
    "To feed this as input to the fully connected layers, they are flattened.\n",
    "\n",
    "To further improve test accuracy and avoid overfitting, Dropout is added. In this model, 10% random neurons will be disabled.\n",
    "\n",
    "To now compute the probability distribution of the 10 classes, a dense layer with units equal to number of classes is added followed by softmax activation layer. The final class for each input will be the one with the highest probability in the softmax layer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 562,826\n",
      "Trainable params: 562,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Sequential CNN\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(32, (5,5), activation='relu', input_shape=(28,28,1)),\n",
    "    Conv2D(32, (5,5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    Flatten(), # flatten out the layers\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(units=10),\n",
    "    \n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "# Initialize and compile:\n",
    "\n",
    "cnn_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a Tensorboard instance\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Batch Size: 64\n",
    "Epochs: 50\n",
    "\n",
    "Validation Size: 20% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 7s 154us/step - loss: 0.1455 - acc: 0.9553 - val_loss: 0.0685 - val_acc: 0.9785\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 5s 111us/step - loss: 0.0434 - acc: 0.9866 - val_loss: 0.0379 - val_acc: 0.9888\n",
      "Epoch 3/100\n",
      "36992/48000 [======================>.......] - ETA: 1s - loss: 0.0305 - acc: 0.9909"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "batch_size = 64\n",
    "history = cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=batch_size,\n",
    "    epochs=100,\n",
    "    verbose=1,\n",
    "    #callbacks=[tensorboard]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cnn_model.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the performance of the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = history.history['acc']\n",
    "val_accuracy = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "\n",
    "plt.plot(epochs, accuracy, 'g', label='Training Accuracy')\n",
    "\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'g', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy is a lot lower than the training accuracy, reducing the number of epochs should be helpful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sequential CNN\n",
    "cnn_model2 = Sequential([\n",
    "    Conv2D(32, (5,5), activation='relu', input_shape=(28,28,1)),\n",
    "    Conv2D(32, (5,5), activation='relu'),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    Flatten(), # flatten out the layers\n",
    "    Dense(units=256, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(units=10),\n",
    "    \n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "# Initialize and compile:\n",
    "\n",
    "cnn_model2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a Tensorboard instance\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "cnn_model2.summary()\n",
    "\n",
    "# Training\n",
    "batch_size = 64\n",
    "history = cnn_model2.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    batch_size=batch_size,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    #callbacks=[tensorboard]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View final accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cnn_model2.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = history.history['acc']\n",
    "val_accuracy = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(accuracy))\n",
    "\n",
    "plt.plot(epochs, accuracy, 'g', label='Training Accuracy')\n",
    "\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'g', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weight and biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = cnn_model2.layers[0].get_weights()[0]\n",
    "biases = cnn_model2.layers[0].get_weights()[1]\n",
    "print(\"Biases: \",biases)\n",
    "\n",
    "print(\"Weights: \",weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some incorrect classifications: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn_model2.predict_classes(x_test)\n",
    "\n",
    "test_labels = [np.where(r==1)[0][0] for r in y_test]\n",
    "flag = 0\n",
    "i = 0\n",
    "\n",
    "while flag<3 and i<len(test_labels):\n",
    "    \n",
    "    if(predictions[i]!=test_labels[i]):\n",
    "        plt.subplot(330 + (flag+1))\n",
    "        plt.imshow(x_train[i].reshape(28,28), cmap=plt.get_cmap('gray'))\n",
    "        plt.xlabel(\"Predicted: \"+str(predictions[i]))\n",
    "        flag+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight into classifier:\n",
    "The classifier performs well across all classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "correct = np.nonzero(predictions==test_labels)[0]\n",
    "incorrect = np.nonzero(predictions!=test_labels)[0]\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "\n",
    "print(classification_report(test_labels, predictions, target_names=target_names))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
